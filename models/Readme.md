This repository would store all your pre-trained or fine-tuned embedding models. Also, we suggest storing your trained models here (.pkl (pickle), .npy (numpy), .hd5 are good methods to store trained models.)

There are the two sources from where you can download the GLoVE Embeddings:
* https://nlp.stanford.edu/projects/glove/
* https://github.com/stanfordnlp/GloVe

If you are interested in converting GLoVE to word2vec, a good resource is https://radimrehurek.com/gensim/scripts/glove2word2vec.html

Download Word2Vec Embeddings: https://radimrehurek.com/gensim/models/word2vec.html

One Stop Shops for Embeddings: 
* https://pypi.org/project/embeddings/ 
* http://vectors.nlpl.eu/repository/
* https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html 

For issues, please email: mgaur@email.sc.edu
